version: '2.3'
services:
  postgres:
    image: postgres:9.6
    restart: always
    volumes:
      - postgres:/srv/airflow-docker/postgresql/data
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airfl0w
      POSTGRES_DB: airflow

  scheduler:
    build:
      context: .
      target: with-spark-optional-dag
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - postgres
    environment:
      USER: airflow
      GROUP: airflow
      AIRFLOW__CORE__FERNET_KEY: 8NE6O6RcTJpxcCkuKxOHOExzIJkXkeJKbRie03a69dI=
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:airfl0w@postgres:5432/airflow
      PYSPARK_SUBMIT_ARGS: --driver-memory 16g --packages com.databricks:spark-csv_2.10:1.5.0,com.databricks:spark-avro_2.10:2.0.1,graphframes:graphframes:0.1.0-spark1.6 pyspark-shell
#      PYSPARK_SUBMIT_ARGS: --driver-memory 16g --packages com.databricks:spark-csv_2.11:1.5.0,com.databricks:spark-avro_2.11:3.1.0,graphframes:graphframes:0.3.0-spark2.0-s_2.11 pyspark-shell
      PYSPARK_PYTHON: /usr/local/bin/python2.7
      PIPELINE_DATA_PATH: hdfs://dsg-cluster-gw01/datasets/finnet
      PIPELINE_DATA_FORMAT: com.databricks.spark.avro
    command: afp-scheduler
    volumes:
      - airflow_logs:/airflow/logs:rw
      - /opt/cloudera/parcels/Anaconda-2019.10/bin/:${SPARK_HOME}/bin/:ro
      - /usr/bin/sqoop:/usr/bin/sqoop:ro

  webserver:
    build:
      context: .
      target: with-spark-optional-dag
      dockerfile: Dockerfile
    restart: always
    depends_on:
      - scheduler
    environment:
      USER: airflow
      GROUP: airflow
      AIRFLOW__CORE__FERNET_KEY: 8NE6O6RcTJpxcCkuKxOHOExzIJkXkeJKbRie03a69dI=
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:airfl0w@postgres:5432/airflow
      AIRFLOW_USER: zerone
      AIRFLOW_EMAIL: info@uzer.one
      AIRFLOW_PASSWORD: _zerone@
    ports:
      - 127.0.0.1:8083:8083
    command: afp-webserver
    volumes_from:
      - scheduler
volumes:
  postgres: {}
  airflow_logs: {}
